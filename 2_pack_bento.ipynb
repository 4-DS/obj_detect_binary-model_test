{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36fd107-9b9d-47c5-8db4-ff993cda5d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729e295-7dbe-4472-817f-d18053b7d2d9",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#2. specify parameters\n",
    "pipeline_params={\n",
    "}\n",
    "step_params={\n",
    "}\n",
    "substep_params={   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb280082-50e9-437a-9f81-1f8910dd437d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#3 define substep interface\n",
    "from sinara.substep import NotebookSubstep, default_param_values, ENV_NAME, PIPELINE_NAME, ZONE_NAME, STEP_NAME, RUN_ID, ENTITY_NAME, ENTITY_PATH, SUBSTEP_NAME\n",
    "\n",
    "substep = NotebookSubstep(pipeline_params, step_params, substep_params, **default_param_values(\"params/step_params.json\"))\n",
    "\n",
    "substep.interface(\n",
    "    \n",
    "    tmp_inputs =\n",
    "    [\n",
    "        { ENTITY_NAME: \"cache_data\" }\n",
    "    ],\n",
    "    \n",
    "    outputs =\n",
    "    [\n",
    "        { ENTITY_NAME: \"bento_service\" }\n",
    "    ]\n",
    ")\n",
    "\n",
    "substep.print_interface_info()\n",
    "\n",
    "substep.exit_in_visualize_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c34fe-a54d-40ff-ba2e-6e72e9769492",
   "metadata": {
    "tags": []
   },
   "source": [
    "![interface 2_pack_bento_interface.drawio](./imgs/2_pack_bento_interface.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a54b0-0ac1-45d3-993c-2389bed143e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#4 get substep.interface\n",
    "tmp_inputs = substep.tmp_inputs()\n",
    "outputs = substep.outputs()\n",
    "\n",
    "print(f\"{tmp_inputs.cache_data=}\")\n",
    "print(f\"{outputs.bento_service=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f57fb6-de4e-401e-a943-1d9eeed6a956",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#5 run spark\n",
    "from sinara.spark import SinaraSpark\n",
    "\n",
    "spark = SinaraSpark.run_session(0)\n",
    "SinaraSpark.ui_url()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4610905-bb77-4f1c-ad3e-dab6b33949d7",
   "metadata": {},
   "source": [
    "### Reading basic parameters from the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43123195-dd88-4a3d-8589-cfd651d264b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "config_fn = osp.join(tmp_inputs.cache_data, 'config.json')\n",
    "\n",
    "with open(config_fn) as f_id:\n",
    "    CONFIG = json.load(f_id)\n",
    "\n",
    "MODEL_NAME = CONFIG[\"train_config_parameters\"][\"MODEL_NAME\"]\n",
    "WORK_DIR = CONFIG['work_dir'] \n",
    "# ONNX_DIR = CONFIG[\"onnx_dir\"]\n",
    "CONFIG_DEPLOY = CONFIG[\"config_deploy\"]\n",
    "DEVICE = CONFIG[\"device\"]\n",
    "CONFIG_MODEL = CONFIG[\"config_model\"]\n",
    "TORCH_MODEL = CONFIG[\"torch_model\"]\n",
    "OUT_ONNX_MODEL = CONFIG[\"out_onnx_model\"]\n",
    "TYPE_EXPORT = CONFIG[\"type_export\"]\n",
    "test_image_path = CONFIG[\"test_image_path\"]\n",
    "\n",
    "OUT_ONNX_MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfee0e60-a365-4a3e-86b2-ae05f8fd4a2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Reading config files and test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca9969-ddc7-4020-9c78-b9f2e7ec676d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(config_fn, 'rb') as f_id:\n",
    "    config = f_id.read()\n",
    "        \n",
    "with open(test_image_path, 'rb') as f_id:\n",
    "    model_test_image = f_id.read()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b7bfe4-dc89-4833-8e1d-4692a228a883",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e24cb18-4374-4423-ae86-dc7097b376ac",
   "metadata": {},
   "source": [
    "#### Initializing the ONNX model via onnxruntime to calculate the result from the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c15953-3ba3-4749-b7a2-8438689c4d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "ort_session = onnxruntime.InferenceSession(OUT_ONNX_MODEL, providers=[\"CUDAExecutionProvider\"])\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "output_name = [out.name for out in ort_session.get_outputs()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a08f4-2aab-4ef0-bb20-890ec3a66fb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ort_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a110c-f99c-4c25-8b23-2c6ade7855f6",
   "metadata": {},
   "source": [
    "### Packaging in bentoservice\n",
    "with TYPE_EXPORT == \"mmdeploy_export\" - Model_YOLOX_Pack from ./bento_service.py is used    \n",
    "with TYPE_EXPORT == \"torch_export\" - Model_YOLOX_NMS_Pack from ./bento_service_nms.py is used    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f067e2b8-3c5b-47e0-8682-3d3b244b8b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sinara.bentoml import save_bentoservice, load_bentoservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503eb8ba-710c-46f6-ba9f-eee96e352086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG_PACK = dict()\n",
    "CONFIG_PACK[\"MAX_SIZE\"] = CONFIG[\"train_config_parameters\"][\"MAX_SIZE\"]\n",
    "CONFIG_PACK[\"Normalize\"] = CONFIG[\"train_config_parameters\"][\"Normalize\"]\n",
    "CONFIG_PACK[\"CLASSES\"] = CONFIG[\"CLASSES\"]\n",
    "CONFIG_PACK[\"train_config_parameters\"] = CONFIG[\"train_config_parameters\"]\n",
    "CONFIG_PACK[\"MIN_OBJECT_SIZE\"] = CONFIG[\"MIN_OBJECT_SIZE\"]\n",
    "CONFIG_PACK[\"type_export\"] = TYPE_EXPORT\n",
    "\n",
    "CONFIG_PACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42057826-64af-4d9b-bf4f-4755f3173814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = json.dumps({\"class_names\": {id_class: class_name for id_class, class_name in enumerate(CONFIG_PACK[\"CLASSES\"])}})\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224090a4-82c4-4663-b1fb-39eaca475c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TYPE_EXPORT == \"mmdeploy_export\":\n",
    "    from bento_service import Model_YOLOX_Pack\n",
    "    \n",
    "    # getting the result from the test image\n",
    "    from bento_service import PrePostProcessing\n",
    "    \n",
    "    pre_post_processing = PrePostProcessing() \n",
    "    input_data, scale_factors = pre_post_processing.prep_processing(test_image_path)\n",
    "    outs = ort_session.run(output_name, {input_name: input_data})\n",
    "    outs = pre_post_processing.post_processing(outs, scale_factors)\n",
    "    # saving test predict result in pickle format\n",
    "    import pickle\n",
    "    with open(osp.join(tmp_inputs.cache_data, 'test_result.pkl'), 'wb') as pkl_file:\n",
    "        pickle.dump(outs, pkl_file)\n",
    "    with open(osp.join(tmp_inputs.cache_data, 'test_result.pkl'), 'rb') as f_id:\n",
    "        test_result = f_id.read() \n",
    "    \n",
    "    # initialize bento service\n",
    "    model_service = Model_YOLOX_Pack()\n",
    "    model_service.pack('model_name', MODEL_NAME)\n",
    "    model_service.pack('class_names', class_names) \n",
    "    serv_v = f\"{outputs.fullname('bento_service')}.{substep.run_id}\"\n",
    "    model_service.pack('service_version', serv_v)\n",
    "    model_service.pack('model', OUT_ONNX_MODEL)\n",
    "    model_service.pack('test_image', model_test_image)\n",
    "    model_service.pack('config', json.dumps(CONFIG_PACK))\n",
    "    model_service.pack('test_result', test_result)    \n",
    "    \n",
    "    # save model as a bento pack\n",
    "    save_bentoservice(model_service, path=outputs.bento_service, service_version=serv_v)\n",
    "    \n",
    "    print(\"pack to bentoservice onnx model converted from mmdeploy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a0d97-1371-40a8-a10b-bd74decad8d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TYPE_EXPORT == \"torch_export\":\n",
    "    from bento_service_nms import Model_YOLOX_Pack\n",
    "    \n",
    "    # getting the result from the test image\n",
    "    from bento_service_nms import PrePostProcessing\n",
    "    \n",
    "    pre_post_processing = PrePostProcessing(num_classes = len(CONFIG_PACK[\"CLASSES\"])) \n",
    "    input_data, scale_factors = pre_post_processing.prep_processing(test_image_path)\n",
    "    outs = ort_session.run(output_name, {input_name: input_data})\n",
    "    outs = pre_post_processing.post_processing(outs, scale_factors)\n",
    "    # saving test predict result in pickle format\n",
    "    import pickle\n",
    "    with open(osp.join(tmp_inputs.cache_data, 'test_result.pkl'), 'wb') as pkl_file:\n",
    "        pickle.dump(outs, pkl_file)\n",
    "    with open(osp.join(tmp_inputs.cache_data, 'test_result.pkl'), 'rb') as f_id:\n",
    "        test_result = f_id.read() \n",
    "        \n",
    "    # initialize bento service\n",
    "    model_service = Model_YOLOX_Pack()    \n",
    "    serv_v = f\"{outputs.fullname('bento_service')}.{substep.run_id}\"\n",
    "    model_service.pack('service_version', serv_v)\n",
    "    model_service.pack('model_name', MODEL_NAME)\n",
    "    model_service.pack('class_names', class_names) \n",
    "    model_service.pack('model', OUT_ONNX_MODEL)\n",
    "    model_service.pack('test_image', model_test_image)\n",
    "    model_service.pack('config', json.dumps(CONFIG_PACK))\n",
    "    model_service.pack('test_result', test_result)    \n",
    "    \n",
    "    # save model as a bento pack\n",
    "    save_bentoservice(model_service, path=outputs.bento_service, service_version=serv_v)\n",
    "    \n",
    "    print(\"pack to bentoservice onnx model converted from torch.export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad34c2-0534-4d31-a20a-309fcee8cb50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#stop spark\n",
    "SinaraSpark.stop_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd780650-1a96-43a1-a6c9-a21a04ffb509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
